"""
Main CLI Interface for Medical Data Integration Platform (MDIP)

This module provides a command-line interface for all MDIP functionality including:
- File analysis and structure discovery
- Data matching and integration
- Quality assessment and reporting
- Configuration management
"""

import argparse
import sys
import os
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any

# Add the src directory to Python path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from mdip.core.matcher import DataMatcher
from mdip.core.quality_control import QualityAssessment
from mdip.core.reporter import ReportGenerator
from mdip.core.validation import MedicalDataValidator
from mdip.config.field_config import FieldConfig
from mdip.utils.data_utils import DataProcessor


def setup_logging(level: str = 'INFO'):
    """Setup logging configuration."""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('mdip.log')
        ]
    )


def analyze_files_command(args):
    """å¤„ç†æ–‡ä»¶åˆ†æå‘½ä»¤ã€‚"""
    print(f"ğŸ” æ­£åœ¨åˆ†æç›®å½•ä¸­çš„æ–‡ä»¶: {args.directory}")
    
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        field_config = FieldConfig()
        matcher = DataMatcher(field_config)
        
        # å‘ç°Excelæ–‡ä»¶
        import glob
        excel_files = glob.glob(os.path.join(args.directory, "*.xlsx")) + glob.glob(os.path.join(args.directory, "*.xls"))
        
        if not excel_files:
            print("âŒ åœ¨æŒ‡å®šç›®å½•ä¸­æœªæ‰¾åˆ°Excelæ–‡ä»¶ã€‚")
            return
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶ï¼š")
        
        # åˆ†ææ¯ä¸ªæ–‡ä»¶
        analysis_results = {}
        for file_path in excel_files:
            filename = os.path.basename(file_path)
            print(f"\nğŸ“„ æ­£åœ¨åˆ†æ: {filename}")
            
            try:
                # ç®€å•çš„Excelæ–‡ä»¶åˆ†æ
                import pandas as pd
                excel_file = pd.ExcelFile(file_path)
                
                file_analysis = {
                    'filename': filename,
                    'sheets': {},
                    'total_fields': 0,
                    'file_size': os.path.getsize(file_path)
                }
                
                for sheet_name in excel_file.sheet_names:
                    try:
                        df = pd.read_excel(file_path, sheet_name=sheet_name, nrows=0)  # åªè¯»å–è¡¨å¤´
                        headers = list(df.columns)
                        file_analysis['sheets'][sheet_name] = {
                            'headers': headers,
                            'field_count': len(headers)
                        }
                        file_analysis['total_fields'] += len(headers)
                    except Exception as e:
                        print(f"      âš ï¸ æ— æ³•è¯»å–å·¥ä½œè¡¨ {sheet_name}: {e}")
                
                analysis_results[file_path] = file_analysis
                
                print(f"   ğŸ“‹ å·¥ä½œè¡¨æ•°é‡: {len(file_analysis['sheets'])}")
                print(f"   ğŸ·ï¸  æ€»å­—æ®µæ•°: {file_analysis['total_fields']}")
                print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {file_analysis['file_size']/1024:.1f} KB")
                
                if args.verbose:
                    for sheet_name, sheet_info in file_analysis['sheets'].items():
                        print(f"      ğŸ“‘ {sheet_name}: {len(sheet_info['headers'])} ä¸ªå­—æ®µ")
                        if hasattr(args, 'show_fields') and args.show_fields:
                            for header in sheet_info['headers'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå­—æ®µ
                                print(f"         - {header}")
                            if len(sheet_info['headers']) > 5:
                                print(f"         ... è¿˜æœ‰ {len(sheet_info['headers']) - 5} ä¸ªå­—æ®µ")
                
            except Exception as e:
                print(f"   âŒ åˆ†ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        
        # Generate analysis report if requested
        if args.output:
            print(f"\nğŸ“ Generating analysis report: {args.output}")
            
            reporter = ReportGenerator()
            
            # Convert analysis results to dataframes for reporting
            dataframes = {}
            for file_path, analysis in analysis_results.items():
                filename = Path(file_path).stem
                # Create a summary dataframe
                sheet_data = []
                for sheet_name, sheet_info in analysis['sheets'].items():
                    sheet_data.append({
                        'File': filename,
                        'Sheet': sheet_name,
                        'Fields': len(sheet_info['headers']),
                        'Records': sheet_info.get('row_count', 'Unknown')
                    })
                
                if sheet_data:
                    import pandas as pd
                    dataframes[filename] = pd.DataFrame(sheet_data)
            
            # Generate report
            report = reporter.generate_data_summary_report(dataframes, "File Analysis Report")
            
            # Export report
            if args.output.endswith('.json'):
                reporter.export_report_to_json(report, args.output)
            else:
                # Default to Excel
                output_path = args.output if args.output.endswith('.xlsx') else f"{args.output}.xlsx"
                reporter.export_report_to_excel(report, output_path)
            
            print(f"âœ… Report saved to: {args.output}")
        
        print(f"\nâœ… Analysis completed for {len(analysis_results)} files.")
        
    except Exception as e:
        print(f"âŒ Error during file analysis: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def match_data_command(args):
    """Handle data matching command."""
    print(f"ğŸ”— Matching data from files: {', '.join(args.files)}")
    
    try:
        # Initialize components
        field_config = FieldConfig()
        matcher = DataMatcher(field_config)
        
        # Load files
        dataframes = {}
        for file_path in args.files:
            if not os.path.exists(file_path):
                print(f"âŒ File not found: {file_path}")
                continue
            
            print(f"ğŸ“‚ Loading: {Path(file_path).name}")
            
            # Load Excel file (use first sheet if not specified)
            import pandas as pd
            if file_path.endswith('.xlsx') or file_path.endswith('.xls'):
                df = pd.read_excel(file_path, sheet_name=0)  # First sheet
            elif file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            else:
                print(f"âš ï¸  Unsupported file format: {file_path}")
                continue
            
            filename = Path(file_path).stem
            dataframes[filename] = df
            print(f"   ğŸ“Š Shape: {df.shape}")
        
        if len(dataframes) < 2:
            print("âŒ Need at least 2 files for matching.")
            return
        
        # Configure matching
        if args.config:
            print(f"ğŸ“„ Using configuration: {args.config}")
            # Load configuration (implementation depends on config format)
        
        # Perform matching
        print("\nğŸ”„ Performing data matching...")
        
        # Use default matching fields if not specified
        matching_fields = args.fields if args.fields else ['patient_id', 'name', 'id']
        
        # Find available matching fields
        available_fields = set()
        for df in dataframes.values():
            available_fields.update(df.columns)
        
        actual_matching_fields = [field for field in matching_fields if field in available_fields]
        
        if not actual_matching_fields:
            print("âš ï¸  No matching fields found. Available fields:")
            for filename, df in dataframes.items():
                print(f"   {filename}: {list(df.columns)[:5]}...")
            return
        
        print(f"ğŸ¯ Using matching fields: {actual_matching_fields}")
        
        # Perform the match (simplified version)
        # In a full implementation, this would use the DataMatcher class methods
        first_df = list(dataframes.values())[0]
        first_name = list(dataframes.keys())[0]
        
        matched_results = {
            'matched_records': first_df,  # Placeholder
            'total_records': sum(len(df) for df in dataframes.values()),
            'matching_fields': actual_matching_fields,
            'match_method': 'exact' if not args.fuzzy else 'fuzzy'
        }
        
        print(f"âœ… Matching completed. Processed {matched_results['total_records']} total records.")
        
        # Generate matching report if requested
        if args.output:
            print(f"ğŸ“ Generating matching report: {args.output}")
            
            reporter = ReportGenerator()
            report = reporter.generate_matching_report(matched_results, "Data Matching Report")
            
            # Export report
            if args.output.endswith('.json'):
                reporter.export_report_to_json(report, args.output)
            else:
                output_path = args.output if args.output.endswith('.xlsx') else f"{args.output}.xlsx"
                reporter.export_report_to_excel(report, output_path)
            
            print(f"âœ… Report saved to: {args.output}")
        
    except Exception as e:
        print(f"âŒ Error during data matching: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def assess_quality_command(args):
    """Handle quality assessment command."""
    print(f"ğŸ” Assessing quality of: {args.file}")
    
    try:
        if not os.path.exists(args.file):
            print(f"âŒ File not found: {args.file}")
            return
        
        # Load file
        print(f"ğŸ“‚ Loading: {Path(args.file).name}")
        
        import pandas as pd
        if args.file.endswith('.xlsx') or args.file.endswith('.xls'):
            df = pd.read_excel(args.file, sheet_name=args.sheet or 0)
        elif args.file.endswith('.csv'):
            df = pd.read_csv(args.file)
        else:
            print(f"âŒ Unsupported file format: {args.file}")
            return
        
        print(f"ğŸ“Š Dataset shape: {df.shape}")
        
        # Initialize quality assessment
        qa = QualityAssessment()
        
        # Configure critical and important fields if provided
        critical_fields = args.critical_fields.split(',') if args.critical_fields else None
        important_fields = args.important_fields.split(',') if args.important_fields else None
        key_fields = args.key_fields.split(',') if args.key_fields else None
        
        # Perform quality assessment
        print("ğŸ”„ Performing quality assessment...")
        
        quality_metrics = qa.generate_overall_assessment(
            df,
            critical_fields=critical_fields,
            important_fields=important_fields,
            key_fields=key_fields
        )
        
        # Display results
        print(f"\nğŸ“ˆ QUALITY ASSESSMENT RESULTS")
        print(f"=" * 50)
        print(f"Overall Quality Score: {quality_metrics.overall_score:.3f}")
        print(f"Quality Grade: {qa._get_quality_grade(quality_metrics.overall_score)}")
        print(f"\nDimension Scores:")
        print(f"  ğŸ“‹ Completeness: {quality_metrics.completeness.get('overall_completeness', 0):.3f}")
        print(f"  ğŸ”„ Consistency:  {quality_metrics.consistency.get('overall_consistency_score', 0):.3f}")
        print(f"  ğŸ¯ Uniqueness:   {quality_metrics.uniqueness.get('uniqueness_score', 0):.3f}")
        print(f"  âœ… Accuracy:     {quality_metrics.accuracy.get('overall_accuracy_score', 0):.3f}")
        print(f"  â° Timeliness:   {quality_metrics.timeliness.get('timeliness_score', 0):.3f}")
        
        # Show key issues
        if quality_metrics.uniqueness.get('duplicate_rate', 0) > 0:
            print(f"\nâš ï¸  Found {quality_metrics.uniqueness.get('duplicate_rows', 0)} duplicate rows "
                  f"({quality_metrics.uniqueness.get('duplicate_rate', 0):.1%})")
        
        if quality_metrics.accuracy.get('invalid_value_count', 0) > 0:
            print(f"âš ï¸  Found {quality_metrics.accuracy.get('invalid_value_count', 0)} data quality issues")
        
        # Generate detailed report if requested
        if args.output:
            print(f"\nğŸ“ Generating detailed quality report: {args.output}")
            
            reporter = ReportGenerator()
            report = reporter.generate_quality_report(quality_metrics.to_dict(), "Quality Assessment Report")
            
            # Export report
            if args.output.endswith('.json'):
                reporter.export_report_to_json(report, args.output)
            else:
                output_path = args.output if args.output.endswith('.xlsx') else f"{args.output}.xlsx"
                reporter.export_report_to_excel(report, output_path)
            
            print(f"âœ… Detailed report saved to: {args.output}")
        
        print(f"\nâœ… Quality assessment completed.")
        
    except Exception as e:
        print(f"âŒ Error during quality assessment: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def validate_data_command(args):
    """Handle data validation command."""
    print(f"âœ… Validating data: {args.file}")
    
    try:
        if not os.path.exists(args.file):
            print(f"âŒ File not found: {args.file}")
            return
        
        # Load file
        print(f"ğŸ“‚ Loading: {Path(args.file).name}")
        
        import pandas as pd
        if args.file.endswith('.xlsx') or args.file.endswith('.xls'):
            df = pd.read_excel(args.file, sheet_name=args.sheet or 0)
        elif args.file.endswith('.csv'):
            df = pd.read_csv(args.file)
        else:
            print(f"âŒ Unsupported file format: {args.file}")
            return
        
        print(f"ğŸ“Š Dataset shape: {df.shape}")
        
        # Initialize validator
        if args.medical:
            print("ğŸ¥ Using medical data validation rules")
            validator = MedicalDataValidator()
        else:
            print("ğŸ“‹ Using general validation rules")
            from mdip.core.validation import DataValidator
            validator = DataValidator()
        
        # Add custom rules if provided
        if args.rules:
            print(f"ğŸ“„ Loading custom rules: {args.rules}")
            # Implementation for loading custom rules would go here
        
        # Perform validation
        print("ğŸ”„ Performing data validation...")
        
        validation_results = validator.validate_dataframe(df, return_details=True)
        
        # Display results
        print(f"\nâœ… VALIDATION RESULTS")
        print(f"=" * 40)
        
        if validation_results['is_valid']:
            print("ğŸ‰ All validation checks passed!")
        else:
            print(f"âš ï¸  Found {validation_results['total_errors']} validation errors")
            print(f"ğŸ“Š Fields with errors: {validation_results['summary']['fields_with_errors']}")
            print(f"ğŸ“ Rows with errors: {validation_results['summary']['rows_with_errors']}")
            
            if args.verbose and validation_results['field_errors']:
                print("\nğŸ” Field Error Details:")
                for field, errors in list(validation_results['field_errors'].items())[:5]:
                    print(f"   {field}: {len(errors)} errors")
                    if args.show_errors:
                        for error in errors[:3]:  # Show first 3 errors
                            print(f"      - {error}")
                        if len(errors) > 3:
                            print(f"      ... and {len(errors) - 3} more")
        
        # Generate validation report if requested
        if args.output:
            print(f"\nğŸ“ Generating validation report: {args.output}")
            
            # Save validation results
            import json
            if args.output.endswith('.json'):
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(validation_results, f, indent=2, default=str)
            else:
                # Convert to Excel format
                output_path = args.output if args.output.endswith('.xlsx') else f"{args.output}.xlsx"
                
                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    # Summary
                    summary_data = [
                        ['Total Errors', validation_results['total_errors']],
                        ['Is Valid', validation_results['is_valid']],
                        ['Total Rows', validation_results['summary']['total_rows']],
                        ['Fields Validated', validation_results['summary']['total_fields_validated']],
                        ['Error Rate', f"{validation_results['summary']['error_rate']:.2%}"]
                    ]
                    summary_df = pd.DataFrame(summary_data, columns=['Metric', 'Value'])
                    summary_df.to_excel(writer, sheet_name='Summary', index=False)
                    
                    # Field errors
                    if validation_results['field_errors']:
                        field_error_data = []
                        for field, errors in validation_results['field_errors'].items():
                            for error in errors:
                                field_error_data.append([field, error])
                        
                        if field_error_data:
                            field_errors_df = pd.DataFrame(field_error_data, columns=['Field', 'Error'])
                            field_errors_df.to_excel(writer, sheet_name='Field Errors', index=False)
            
            print(f"âœ… Validation report saved to: {args.output}")
        
        print(f"\nâœ… Validation completed.")
        
    except Exception as e:
        print(f"âŒ Error during validation: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def main():
    """CLIä¸»å…¥å£ç‚¹ã€‚"""
    parser = argparse.ArgumentParser(
        description="åŒ»ç–—æ•°æ®æ•´åˆå¹³å° (MDIP) - å‘½ä»¤è¡Œç•Œé¢",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åˆ†æç›®å½•ä¸­çš„Excelæ–‡ä»¶
  mdip analyze /path/to/data --output åˆ†ææŠ¥å‘Š.xlsx
  
  # åŒ¹é…å¤šä¸ªæ–‡ä»¶çš„æ•°æ®  
  mdip match æ–‡ä»¶1.xlsx æ–‡ä»¶2.csv --fields patient_id,name --fuzzy --output åŒ¹é…æŠ¥å‘Š.xlsx
  
  # è¯„ä¼°æ•°æ®è´¨é‡
  mdip quality æ•°æ®.xlsx --critical-fields patient_id,name --output è´¨é‡æŠ¥å‘Š.json
  
  # éªŒè¯åŒ»ç–—æ•°æ®
  mdip validate æ‚£è€…æ•°æ®.xlsx --medical --output éªŒè¯æŠ¥å‘Š.xlsx
        """
    )
    
    # å…¨å±€é€‰é¡¹
    parser.add_argument('--verbose', '-v', action='store_true', help='å¯ç”¨è¯¦ç»†è¾“å‡º')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='è®¾ç½®æ—¥å¿—çº§åˆ«')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†ææ–‡ä»¶ç»“æ„å’Œå†…å®¹')
    analyze_parser.add_argument('directory', nargs='?', default='docs', help='åŒ…å«è¦åˆ†ææ–‡ä»¶çš„ç›®å½• (é»˜è®¤: docs)')
    analyze_parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    analyze_parser.add_argument('--show-fields', action='store_true', help='åœ¨è¾“å‡ºä¸­æ˜¾ç¤ºå­—æ®µå')
    analyze_parser.set_defaults(func=analyze_files_command)
    
    # åŒ¹é…å‘½ä»¤
    match_parser = subparsers.add_parser('match', help='åŒ¹é…å¹¶æ•´åˆå¤šä¸ªæ–‡ä»¶çš„æ•°æ®')
    match_parser.add_argument('files', nargs='+', help='è¦åŒ¹é…çš„æ–‡ä»¶')
    match_parser.add_argument('--fields', '-f', help='é€—å·åˆ†éš”çš„åŒ¹é…å­—æ®µ')
    match_parser.add_argument('--fuzzy', action='store_true', help='ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…')
    match_parser.add_argument('--config', '-c', help='åŒ¹é…é…ç½®æ–‡ä»¶')
    match_parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    match_parser.set_defaults(func=match_data_command)
    
    # è´¨é‡å‘½ä»¤
    quality_parser = subparsers.add_parser('quality', help='è¯„ä¼°æ•°æ®è´¨é‡')
    quality_parser.add_argument('file', help='è¦è¯„ä¼°çš„æ–‡ä»¶')
    quality_parser.add_argument('--sheet', help='Excelå·¥ä½œè¡¨åç§°/ç´¢å¼•')
    quality_parser.add_argument('--critical-fields', help='é€—å·åˆ†éš”çš„å…³é”®å­—æ®µå')
    quality_parser.add_argument('--important-fields', help='é€—å·åˆ†éš”çš„é‡è¦å­—æ®µå') 
    quality_parser.add_argument('--key-fields', help='é€—å·åˆ†éš”çš„ä¸»é”®å­—æ®µå')
    quality_parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    quality_parser.set_defaults(func=assess_quality_command)
    
    # éªŒè¯å‘½ä»¤
    validate_parser = subparsers.add_parser('validate', help='æ ¹æ®è§„åˆ™éªŒè¯æ•°æ®')
    validate_parser.add_argument('file', help='è¦éªŒè¯çš„æ–‡ä»¶')
    validate_parser.add_argument('--sheet', help='Excelå·¥ä½œè¡¨åç§°/ç´¢å¼•')
    validate_parser.add_argument('--medical', action='store_true', help='ä½¿ç”¨åŒ»ç–—æ•°æ®éªŒè¯è§„åˆ™')
    validate_parser.add_argument('--rules', '-r', help='è‡ªå®šä¹‰éªŒè¯è§„åˆ™æ–‡ä»¶')
    validate_parser.add_argument('--show-errors', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯')
    validate_parser.add_argument('--output', '-o', help='è¾“å‡ºéªŒè¯æŠ¥å‘Šæ–‡ä»¶')
    validate_parser.set_defaults(func=validate_data_command)
    
    # Parse arguments
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(args.log_level)
    
    # Check if command was provided
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Execute command
    try:
        args.func(args)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Operation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Unexpected error: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()